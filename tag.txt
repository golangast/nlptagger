package main

import (
	"encoding/gob"
	"encoding/json"
	"fmt"
	"io/ioutil"
	"math"
	"math/rand/v2"
	"os"
	"strings"

	"github.com/golangast/nlptagger/tagger/tag"
)

// Global variables for vocabularies
var tokenVocab map[string]int
var posTagVocab map[string]int

// Structure to represent training data in JSON
type TrainingDataJSON struct {
	Sentences []tag.Tag `json:"sentences"`
}

func main() {
	if _, err := os.Stat("trained_model.gob"); err == nil {
		// If the model file exists, delete it
		err = os.Remove("trained_model.gob")
		if err != nil {
			fmt.Println("Error deleting model file:", err)
			return
		}
	}
	// 1. Load training data from JSON file
	trainingData, err := loadTrainingDataFromJSON("data/training_data.json")
	if err != nil {
		fmt.Println("Error loading training data:", err)
		return
	}

	// 2. Create vocabularies
	tokenVocab = createTokenVocab(trainingData.Sentences)
	posTagVocab = createPosTagVocab(trainingData.Sentences)

	// Load the trained model (if it exists)
	nn, err := loadModelFromGOB("trained_model.gob")
	if err != nil {
		// If the model file doesn't exist or there's an error loading it,
		// create a new model and train it.
		fmt.Println("Error loading model, creating a new one:", err)
		inputSize := len(tokenVocab)
		hiddenSize := 5 // Adjust as needed
		outputSize := len(posTagVocab)
		nn = NewSimpleNN(inputSize, hiddenSize, outputSize)

		// Train the network
		epochs := 100       // Adjust as needed
		learningRate := 0.1 // Adjust as needed
		accuracy := nn.Train(trainingData.Sentences, epochs, learningRate)
		fmt.Printf("Final Accuracy: %.2f%%\n", accuracy*100)
		// Save the newly trained model
		err = saveModelToGOB(nn, "trained_model.gob")
		if err != nil {
			fmt.Println("Error saving model:", err)
			return
		}
		fmt.Println("New model saved to trained_model.gob")
	} else {
		fmt.Println("Loaded model from trained_model.gob")
	}

	// 3. Create and train the neural network
	inputSize := len(tokenVocab)

	// Train the network
	epochs := 100       // Adjust as needed
	learningRate := 0.1 // Adjust as needed
	nn.Train(trainingData.Sentences, epochs, learningRate)
	// Save the trained model
	err = saveModelToGOB(nn, "trained_model.gob")
	if err != nil {
		fmt.Println("Error saving model:", err)
		return
	}

	fmt.Println("Model saved to trained_model.gob")

	// 4. Example prediction (using a new sentence)
	sentence := "generate a webserver with the handler dog with the data structure people"
	tokens := strings.Fields(sentence) // Tokenize the sentence

	var predictedTags []string
	for _, token := range tokens {
		tokenIndex, ok := tokenVocab[token]
		if !ok {
			fmt.Printf("Token '%s' not found in vocabulary\n", token)
			predictedTags = append(predictedTags, "UNK")
			continue
		}

		inputs := make([]float64, inputSize)
		inputs[tokenIndex] = 1

		predictedOutput := nn.Predict(inputs)
		predictedTagIndex := maxIndex(predictedOutput)
		predictedTag, ok := indexToPosTag(posTagVocab, predictedTagIndex)
		if !ok {
			fmt.Printf("Tag index %d not found in vocabulary\n", predictedTagIndex)
			predictedTags = append(predictedTags, "UNK")
			continue
		}

		predictedTags = append(predictedTags, predictedTag)
	}

	fmt.Println("Sentence:", sentence)
	fmt.Println("Predicted Tags:", strings.Join(predictedTags, " "))
	// 5. Example prediction (using a new sentence)
	//command to tag
	//t := tagger.Tagging("generate a webserver that has the handler named dog with the data structure pedagree")
	//print the tags

	for _, token := range tokens {
		tokenIndex, ok := tokenVocab[token]
		if !ok {
			fmt.Printf("Token '%s' not found in vocabulary\n", token)
			predictedTags = append(predictedTags, "UNK")
			continue
		}

		inputs := make([]float64, nn.InputSize)
		inputs[tokenIndex] = 1

		predictedOutput := nn.Predict(inputs)
		predictedTagIndex := maxIndex(predictedOutput)
		predictedTag, ok := indexToPosTag(posTagVocab, predictedTagIndex)
		if !ok {
			fmt.Printf("Tag index %d not found in vocabulary\n", predictedTagIndex)
			predictedTags = append(predictedTags, "UNK")
			continue
		}

		predictedTags = append(predictedTags, predictedTag)
	}

	// Create a tag.Tag struct with the predicted tags
	predictedTagStruct := tag.Tag{
		Tokens: tokens,
		PosTag: predictedTags, // Use predictedTags for the Type field
	}

	// Print the predicted tag types along with the original sentence
	fmt.Println("Sentence:", sentence)
	fmt.Println("Predicted Tag Types:", strings.Join(predictedTagStruct.PosTag, " "))

	// Access and print individual tag types
	for i, token := range predictedTagStruct.Tokens {
		fmt.Printf("Token: %s, Predicted Tag Type: %s\n", token, predictedTagStruct.PosTag[i])
	}
	// 2. Train the network (add training logic here)
	// This would involve:
	//   - Iterating through your training data (tokens and POS tags)
	//   - Converting tokens to one-hot encoded inputs
	//   - Feeding inputs to the network to get predictions
	//   - Calculating the error between predictions and actual POS tags
	//   - Updating the network's weights using backpropagation
	//   - Repeating the process for multiple epochs
	// ... (Training loop and backpropagation implementation)
	// Define posTagVocab as a slice

}

// Function to load training data from a JSON file
func loadTrainingDataFromJSON(filePath string) (*TrainingDataJSON, error) {
	file, err := os.Open(filePath)
	if err != nil {
		return nil, err
	}
	defer file.Close()

	data, err := ioutil.ReadAll(file)
	if err != nil {
		return nil, err
	}

	var trainingData TrainingDataJSON
	err = json.Unmarshal(data, &trainingData)
	if err != nil {
		return nil, err
	}

	return &trainingData, nil
}
func maxIndex(values []float64) int {
	maxIndex := 0
	maxValue := values[0]
	for i, val := range values {
		if val > maxValue {
			maxValue = val
			maxIndex = i
		}
	}
	return maxIndex
}
func indexToPosTag(posTagVocab map[string]int, predictedTagIndex int) (string, bool) {
	for tag, index := range posTagVocab {
		if index == predictedTagIndex {
			return tag, true
		}
	}
	return "", false
}

func createTokenVocab(trainingData []tag.Tag) map[string]int {
	tokenVocab := make(map[string]int)
	tokenVocab["UNK"] = 0 // Add "UNK" token initially
	index := 1
	for _, sentence := range trainingData { // Iterate through tag.Tag slice
		for _, token := range sentence.Tokens {
			if _, ok := tokenVocab[token]; !ok {
				tokenVocab[token] = index
				index++
			}
		}
	}

	// If index exceeded vocabulary size
	if index > len(tokenVocab)-1 { // Dynamically determine vocabulary size
		// Handle unknown tokens
		tokenVocab["UNK"] = len(tokenVocab) // Add "UNK" token
		index = len(tokenVocab)             // Update index to reflect new vocabulary size

		// Optionally, print a warning message
		//fmt.Println("Warning: Vocabulary size exceeded. Adding 'UNK' token.")
	}

	return tokenVocab
}

// Training function
func (nn *SimpleNN) Train(trainingData []tag.Tag, epochs int, learningRate float64) float64 {
	var accuracy float64
	for epoch := 0; epoch < epochs; epoch++ {
		for _, taggedSentence := range trainingData {
			for i := range taggedSentence.Tokens {
				token := taggedSentence.Tokens[i]
				targetTag := taggedSentence.PosTag[i]
				correctPredictions := 0
				totalPredictions := 0

				// Convert token to one-hot encoded input
				inputs := make([]float64, nn.InputSize)
				tokenVocab := createTokenVocab(trainingData)
				tokenIndex, ok := tokenVocab[token]
				if ok {
					//fmt.Printf("Token: %s, Token Index: %d, Input Size: %d\n", token, tokenIndex, len(inputs))
					if _, ok := tokenVocab[token]; !ok {
						fmt.Printf("Token '%s' not found in vocabulary!\n", token)
					}
					inputs[tokenIndex] = 1
				}

				// Forward pass
				outputs := nn.Predict(inputs)

				// Calculate error
				targetOutput := make([]float64, nn.OutputSize)
				posTagVocab := createPosTagVocab(trainingData)
				targetTagIndex, ok := posTagVocab[targetTag]
				if ok {
					targetOutput[targetTagIndex] = 1
				}
				errors := make([]float64, nn.OutputSize)
				for i := range errors {
					errors[i] = targetOutput[i] - outputs[i]
				}

				// Backpropagation
				// 1. Output layer weight updates
				for i := 0; i < nn.OutputSize; i++ {
					gradient := errors[i] * outputs[i] * (1 - outputs[i]) // Sigmoid derivative
					for j := 0; j < nn.HiddenSize; j++ {
						nn.WeightsHO[i][j] += learningRate * gradient * outputs[j] //Hidden Layer is input to Output Layer
					}
				}

				// 2. Hidden layer weight updates
				hiddenErrors := make([]float64, nn.HiddenSize)
				for i := 0; i < nn.HiddenSize; i++ {
					errorSum := 0.0
					for j := 0; j < nn.OutputSize; j++ {
						errorSum += errors[j] * nn.WeightsHO[j][i]
					}
					hiddenErrors[i] = errorSum * outputs[i] * (1 - outputs[i]) // Sigmoid derivative
				}
				for i := 0; i < nn.HiddenSize; i++ {
					for j := 0; j < nn.InputSize; j++ {
						nn.WeightsIH[i][j] += learningRate * hiddenErrors[i] * inputs[j] //Input Layer is input to Hidden Layer
					}
				}
				for _, taggedSentence := range trainingData {
					for i := range taggedSentence.Tokens {
						// ... (your existing code for forward pass, error calculation, backpropagation)

						// Accuracy Calculation
						predictedOutput := nn.Predict(inputs)
						predictedTagIndex := maxIndex(predictedOutput)
						predictedTag, _ := indexToPosTag(posTagVocab, predictedTagIndex) // Assuming indexToPosTag handles unknown tags

						if predictedTag == taggedSentence.PosTag[i] {
							correctPredictions++
						}
						totalPredictions++
					}
				}

				accuracy = float64(correctPredictions) / float64(totalPredictions)
				//fmt.Printf("Epoch %d: Accuracy = %.2f%%\n", epoch+1, accuracy*100)
			}

		}

	}
	return accuracy
}

func createPosTagVocab(trainingData []tag.Tag) map[string]int {
	posTagVocab := make(map[string]int)
	index := 0

	for _, taggedSentence := range trainingData {
		for _, posTag := range taggedSentence.PosTag {
			if _, ok := posTagVocab[posTag]; !ok { // Check if POS tag is already in the vocabulary
				posTagVocab[posTag] = index
				index++
			}
		}
	}

	return posTagVocab
}

// Forward pass
func (nn *SimpleNN) Predict(inputs []float64) []float64 {
	hidden := make([]float64, nn.HiddenSize)
	for i := range hidden {
		sum := 0.0
		for j := range inputs {
			sum += nn.WeightsIH[i][j] * inputs[j]
		}
		hidden[i] = sigmoid(sum)
	}

	output := make([]float64, nn.OutputSize)
	for i := range output {
		sum := 0.0
		for j := range hidden {
			sum += nn.WeightsHO[i][j] * hidden[j]
		}
		output[i] = sigmoid(sum)
	}

	return output
}

// Activation function (sigmoid)
func sigmoid(x float64) float64 {
	return 1 / (1 + math.Exp(-x))
}
func PrintTable(tokens, pos, ner, phrase, dependency []string, message string) {
	fmt.Println("Token Index | Token        | pos Tag       | ner Tag       | dependency Tag      | phrase Tag")
	fmt.Println("------------|--------------|---------------|---------------|-----------------|--------------")

	// Print table rows
	for i := range tokens {
		fmt.Printf("%-12d| %-13s| %-14s| %-14s| %-16s| %-10s\n", i,
			tokens[i],
			pos[i],
			ner[i],
			dependency[i],
			phrase[i])
		fmt.Println(strings.Repeat("-", 90))

	}
	fmt.Println(message)

}

type SimpleNN struct {
	InputSize  int
	HiddenSize int
	OutputSize int
	WeightsIH  [][]float64 // Input to hidden weights
	WeightsHO  [][]float64 // Hidden to output weights
}

func NewSimpleNN(inputSize, hiddenSize, outputSize int) *SimpleNN {
	nn := &SimpleNN{
		InputSize:  inputSize,
		HiddenSize: hiddenSize,
		OutputSize: outputSize,
	}
	nn.WeightsIH = make([][]float64, hiddenSize)
	for i := range nn.WeightsIH {
		nn.WeightsIH[i] = make([]float64, inputSize)
		for j := range nn.WeightsIH[i] {
			nn.WeightsIH[i][j] = rand.Float64()*2 - 1 // Initialize with random weights
		}
	}
	nn.WeightsHO = make([][]float64, outputSize)
	for i := range nn.WeightsHO {
		nn.WeightsHO[i] = make([]float64, hiddenSize)
		for j := range nn.WeightsHO[i] {
			nn.WeightsHO[i][j] = rand.Float64()*2 - 1
		}
	}
	return nn
}
func saveModelToGOB(model *SimpleNN, filePath string) error {
	file, err := os.Create(filePath)
	if err != nil {
		return err
	}
	defer file.Close()

	encoder := gob.NewEncoder(file)
	err = encoder.Encode(model)
	if err != nil {
		return err
	}

	return nil
}
func loadModelFromGOB(filePath string) (*SimpleNN, error) {
	file, err := os.Open(filePath)
	if err != nil {
		return nil, err
	}
	defer file.Close()

	decoder := gob.NewDecoder(file)
	var model SimpleNN
	err = decoder.Decode(&model)
	if err != nil {
		return nil, err
	}

	return &model, nil
}



#ffff









package main

import (
	"encoding/gob"
	"encoding/json"
	"fmt"
	"io/ioutil"
	"math"
	"math/rand/v2"
	"os"
	"strings"

	"github.com/golangast/nlptagger/tagger/tag"
)

// Global variables for vocabularies
var tokenVocab map[string]int
var posTagVocab map[string]int
var nerTagVocab map[string]int

// Structure to represent training data in JSON
type TrainingDataJSON struct {
	Sentences []tag.Tag `json:"sentences"`
}

func main() {
	if _, err := os.Stat("trained_model.gob"); err == nil {
		// If the model file exists, delete it
		err = os.Remove("trained_model.gob")
		if err != nil {
			fmt.Println("Error deleting model file:", err)
			return
		}
	}
	// 1. Load training data from JSON file
	trainingData, err := loadTrainingDataFromJSON("data/training_data.json")
	if err != nil {
		fmt.Println("Error loading training data:", err)
		return
	}

	// 2. Create vocabularies
	tokenVocab = createTokenVocab(trainingData.Sentences)
	posTagVocab = createPosTagVocab(trainingData.Sentences)
	nerTagVocab = createNerTagVocab(trainingData.Sentences)

	// Load the trained model (if it exists)
	nn, err := loadModelFromGOB("trained_model.gob")
	if err != nil {
		// If the model file doesn't exist or there's an error loading it,
		// create a new model and train it.
		fmt.Println("Error loading model, creating a new one:", err)
		inputSize := len(tokenVocab)
		hiddenSize := 5 // Adjust as needed
		outputSize := len(posTagVocab)
		nn = NewSimpleNN(inputSize, hiddenSize, outputSize)

		// Train the network
		epochs := 100       // Adjust as needed
		learningRate := 0.1 // Adjust as needed
		posAccuracy, nerAccuracy := nn.Train(trainingData.Sentences, epochs, learningRate)
		fmt.Printf("Final POS Accuracy: %.2f%%\n", posAccuracy*100)
		fmt.Printf("Final Ner Accuracy: %.2f%%\n", nerAccuracy*100)

		// Save the newly trained model
		err = saveModelToGOB(nn, "trained_model.gob")
		if err != nil {
			fmt.Println("Error saving model:", err)
			return
		}
		fmt.Println("New model saved to trained_model.gob")
	} else {
		fmt.Println("Loaded model from trained_model.gob")
	}

	// 3. Create and train the neural network
	inputSize := len(tokenVocab)

	// Train the network
	epochs := 100       // Adjust as needed
	learningRate := 0.1 // Adjust as needed
	nn.Train(trainingData.Sentences, epochs, learningRate)
	// Save the trained model
	err = saveModelToGOB(nn, "trained_model.gob")
	if err != nil {
		fmt.Println("Error saving model:", err)
		return
	}

	fmt.Println("Model saved to trained_model.gob")

	// 4. Example prediction (using a new sentence)
	sentence := "generate a webserver with the handler dog with the data structure people"
	tokens := strings.Fields(sentence) // Tokenize the sentence
	var predictedNerTags []string
	var predictedTags []string
	for _, token := range tokens {
		tokenIndex, ok := tokenVocab[token]
		if !ok {
			fmt.Printf("Token '%s' not found in vocabulary\n", token)
			predictedTags = append(predictedTags, "UNK")
			continue
		}

		inputs := make([]float64, inputSize)
		inputs[tokenIndex] = 1

		predictedOutput := nn.Predict(inputs)
		predictedTagIndex := maxIndex(predictedOutput)
		predictedTag, ok := indexToPosTag(posTagVocab, predictedTagIndex)
		if !ok {
			fmt.Printf("Tag index %d not found in vocabulary\n", predictedTagIndex)
			predictedTags = append(predictedTags, "UNK")
			continue
		}

		predictedTags = append(predictedTags, predictedTag)

		// NER Tag Prediction
		predictedNerTag, ok := indexToNerTag(nerTagVocab, predictedTagIndex)
		if !ok {
			fmt.Printf("NER Tag index %d not found in vocabulary\n", predictedTagIndex)
			predictedNerTags = append(predictedNerTags, "UNK")
			continue
		}

		predictedNerTags = append(predictedNerTags, predictedNerTag) // Store predicted NER tags

	}

	if len(predictedNerTags) < len(tokens) {
		paddingSize := len(tokens) - len(predictedNerTags)
		padding := make([]string, paddingSize)
		predictedNerTags = append(predictedNerTags, padding...)
	}

	fmt.Println("Sentence:", sentence)
	fmt.Println("Predicted POS Tags:", strings.Join(predictedTags, " "))
	fmt.Println("Predicted Ner Tags:", strings.Join(predictedNerTags, " "))

	for _, token := range tokens {
		tokenIndex, ok := tokenVocab[token]
		if !ok {
			fmt.Printf("Token '%s' not found in vocabulary\n", token)
			predictedTags = append(predictedTags, "UNK")
			continue
		}

		inputs := make([]float64, nn.InputSize)
		inputs[tokenIndex] = 1

		predictedOutput := nn.Predict(inputs)
		predictedTagIndex := maxIndex(predictedOutput)
		predictedTag, ok := indexToPosTag(posTagVocab, predictedTagIndex)
		if !ok {
			fmt.Printf("Tag index %d not found in vocabulary\n", predictedTagIndex)
			predictedTags = append(predictedTags, "UNK")
			continue
		}

		predictedTags = append(predictedTags, predictedTag)
	}

	// Create a tag.Tag struct with the predicted tags
	predictedTagStruct := tag.Tag{
		Tokens: tokens,
		PosTag: predictedTags, // Use predictedTags for the Type field
		NerTag: predictedNerTags,
	}

	// Print the predicted tag types along with the original sentence
	fmt.Println("Sentence:", sentence)
	fmt.Println("Predicted POS Tag Types:", strings.Join(predictedTagStruct.PosTag, " "))
	fmt.Println("Predicted NER Tag Types:", strings.Join(predictedTagStruct.NerTag, " "))
	if len(predictedTagStruct.Tokens) != len(predictedTagStruct.NerTag) || len(predictedTagStruct.Tokens) != len(predictedTagStruct.PosTag) {
		// Handle data length mismatch, for example, by printing an error message
		fmt.Println("Error: Data length mismatch in predictedTagStruct")

	}
	// Access and print individual tag types
	for i, token := range predictedTagStruct.Tokens {
		fmt.Printf("Token: %s, Predicted POS Tag Type: %s\n", token, predictedTagStruct.PosTag[i])
		if predictedTagStruct.NerTag[i] == "" {
			predictedTagStruct.NerTag[i] = "UNK"
			fmt.Printf("Token: %s, Predicted NER Tag Type: %s\n", token, predictedTagStruct.NerTag[i])

		} else {
			fmt.Printf("Token: %s, Predicted NER Tag Type: %s\n", token, predictedTagStruct.NerTag[i])
		}

	}

}
func createNerTagVocab(trainingData []tag.Tag) map[string]int {
	nerTagVocab := make(map[string]int)
	index := 0

	for _, taggedSentence := range trainingData {
		for _, nerTag := range taggedSentence.NerTag { // Access NerTag field
			if _, ok := nerTagVocab[nerTag]; !ok {
				nerTagVocab[nerTag] = index
				index++
			}
		}
	}

	return nerTagVocab
}

// Function to convert index to NER tag (similar to indexToPosTag)
func indexToNerTag(nerTagVocab map[string]int, predictedTagIndex int) (string, bool) {
	for tag, index := range nerTagVocab {
		if index == predictedTagIndex {
			return tag, true
		}
	}
	return "", false
}

// Function to load training data from a JSON file
func loadTrainingDataFromJSON(filePath string) (*TrainingDataJSON, error) {
	file, err := os.Open(filePath)
	if err != nil {
		return nil, err
	}
	defer file.Close()

	data, err := ioutil.ReadAll(file)
	if err != nil {
		return nil, err
	}

	var trainingData TrainingDataJSON
	err = json.Unmarshal(data, &trainingData)
	if err != nil {
		return nil, err
	}

	return &trainingData, nil
}
func maxIndex(values []float64) int {
	maxIndex := 0
	maxValue := values[0]
	for i, val := range values {
		if val > maxValue {
			maxValue = val
			maxIndex = i
		}
	}
	return maxIndex
}
func indexToPosTag(posTagVocab map[string]int, predictedTagIndex int) (string, bool) {
	for tag, index := range posTagVocab {
		if index == predictedTagIndex {
			return tag, true
		}
	}
	return "", false
}

func createTokenVocab(trainingData []tag.Tag) map[string]int {
	tokenVocab := make(map[string]int)
	tokenVocab["UNK"] = 0 // Add "UNK" token initially
	index := 1
	for _, sentence := range trainingData { // Iterate through tag.Tag slice
		for _, token := range sentence.Tokens {
			if _, ok := tokenVocab[token]; !ok {
				tokenVocab[token] = index
				index++
			}
		}
	}

	// If index exceeded vocabulary size
	if index > len(tokenVocab)-1 { // Dynamically determine vocabulary size
		// Handle unknown tokens
		tokenVocab["UNK"] = len(tokenVocab) // Add "UNK" token
		index = len(tokenVocab)             // Update index to reflect new vocabulary size

		// Optionally, print a warning message
		//fmt.Println("Warning: Vocabulary size exceeded. Adding 'UNK' token.")
	}

	return tokenVocab
}

// Training function
func (nn *SimpleNN) Train(trainingData []tag.Tag, epochs int, learningRate float64) (float64, float64) {
	// Create vocabularies outside the training loop
	tokenVocab := createTokenVocab(trainingData)
	posTagVocab := createPosTagVocab(trainingData)
	nerTagVocab := createNerTagVocab(trainingData)

	// Create reverse vocabularies
	reversePosTagVocab := make(map[int]string, len(posTagVocab))
	for tag, index := range posTagVocab {
		reversePosTagVocab[index] = tag
	}
	reverseNerTagVocab := make(map[int]string, len(nerTagVocab))
	for tag, index := range nerTagVocab {
		reverseNerTagVocab[index] = tag
	}

	for epoch := 0; epoch < epochs; epoch++ {
		for _, taggedSentence := range trainingData {
			for i := range taggedSentence.Tokens {
				token := taggedSentence.Tokens[i]
				targetPosTag := taggedSentence.PosTag[i]
				targetNerTag := taggedSentence.NerTag[i]

				// Convert token to one-hot encoded input
				inputs := make([]float64, nn.InputSize)
				tokenIndex, ok := tokenVocab[token]
				if ok {
					inputs[tokenIndex] = 1
				}

				// Forward pass (shared for POS and NER)
				outputs := nn.Predict(inputs)

				// Calculate error and backpropagate for POS tagging
				posTargetOutput := make([]float64, nn.OutputSize)
				posTagIndex, ok := posTagVocab[targetPosTag]
				if ok {
					posTargetOutput[posTagIndex] = 1
				}
				posErrors := make([]float64, nn.OutputSize)
				for j := range posErrors {
					posErrors[j] = posTargetOutput[j] - outputs[j]
				}
				nn.Backpropagate(inputs, posTargetOutput, learningRate) // Use posTargetOutput

				// Calculate error and backpropagate for NER tagging
				nerTargetOutput := make([]float64, nn.OutputSize)
				nerTagIndex, ok := nerTagVocab[targetNerTag]
				if ok {
					nerTargetOutput[nerTagIndex] = 1
				}
				nerErrors := make([]float64, nn.OutputSize)
				for j := range nerErrors {
					nerErrors[j] = nerTargetOutput[j] - outputs[j]
				}
				nn.Backpropagate(inputs, nerTargetOutput, learningRate) // Use nerTargetOutput
			}
		}

	}
	// Inside the Train function
	tposAccuracy, tnerAccuracy := calculateAccuracy(trainingData, nn, reversePosTagVocab, reverseNerTagVocab)
	return tposAccuracy, tnerAccuracy
}
func (nn *SimpleNN) Backpropagate(inputs, targets []float64, learningRate float64) {
	// 1. Calculate output layer errors
	outputErrors := make([]float64, nn.OutputSize)
	for i := 0; i < nn.OutputSize; i++ {
		outputErrors[i] = targets[i] - nn.Outputs[i]
	}

	// 2. Calculate hidden layer errors
	hiddenErrors := make([]float64, nn.HiddenSize)
	for i := 0; i < nn.HiddenSize; i++ {
		errorSum := 0.0
		for j := 0; j < nn.OutputSize; j++ {
			errorSum += outputErrors[j] * nn.WeightsHO[j][i]
		}
		hiddenErrors[i] = errorSum * nn.HiddenOutputs[i] * (1 - nn.HiddenOutputs[i]) // Sigmoid derivative
	}

	// 3. Update output layer weights
	for i := 0; i < nn.OutputSize; i++ {
		for j := 0; j < nn.HiddenSize; j++ {
			nn.WeightsHO[i][j] += learningRate * outputErrors[i] * nn.HiddenOutputs[j]
		}
	}

	// 4. Update hidden layer weights
	for i := 0; i < nn.HiddenSize; i++ {
		for j := 0; j < nn.InputSize; j++ {
			nn.WeightsIH[i][j] += learningRate * hiddenErrors[i] * inputs[j]
		}
	}
}
func calculateAccuracy(trainingData []tag.Tag, nn *SimpleNN, reversePosTagVocab, reverseNerTagVocab map[int]string) (float64, float64) {
	posAccuracy := 0.0
	nerAccuracy := 0.0
	posCorrectPredictions := 0
	nerCorrectPredictions := 0
	totalPredictions := 0

	for _, taggedSentence := range trainingData {
		for i := range taggedSentence.Tokens {
			token := taggedSentence.Tokens[i]

			// Convert token to one-hot encoded input
			inputs := make([]float64, nn.InputSize)
			tokenIndex, ok := tokenVocab[token]
			if ok {
				inputs[tokenIndex] = 1
			}

			// Forward pass
			predictedOutput := nn.Predict(inputs)
			predictedTagIndex := maxIndex(predictedOutput)

			for _, taggedSentence := range trainingData {
				for i := range taggedSentence.Tokens {
					// ... (forward pass and prediction code) ...

					// Get predicted tag labels using reverse vocabularies
					predictedPosTag, _ := reversePosTagVocab[predictedTagIndex]
					predictedNerTag, _ := reverseNerTagVocab[predictedTagIndex]

					// Compare predicted tags with actual tags
					if predictedPosTag == taggedSentence.PosTag[i] {
						posCorrectPredictions++
					}
					if predictedNerTag == taggedSentence.NerTag[i] {
						nerCorrectPredictions++
					}
					totalPredictions++

				}
			}
		}
	}
	// Calculate accuracy for POS and NER tags
	posAccuracy = float64(posCorrectPredictions) / float64(totalPredictions)
	nerAccuracy = float64(nerCorrectPredictions) / float64(totalPredictions)

	return posAccuracy, nerAccuracy
}
func createPosTagVocab(trainingData []tag.Tag) map[string]int {
	posTagVocab := make(map[string]int)
	index := 0

	for _, taggedSentence := range trainingData {
		for _, posTag := range taggedSentence.PosTag {
			if _, ok := posTagVocab[posTag]; !ok { // Check if POS tag is already in the vocabulary
				posTagVocab[posTag] = index
				index++
			}
		}
	}

	return posTagVocab
}

// Forward pass
func (nn *SimpleNN) Predict(inputs []float64) []float64 {
	hidden := make([]float64, nn.HiddenSize)
	for i := range hidden {
		sum := 0.0
		for j := range inputs {
			sum += nn.WeightsIH[i][j] * inputs[j]
		}
		hidden[i] = sigmoid(sum)
	}

	output := make([]float64, nn.OutputSize)
	for i := range output {
		sum := 0.0
		for j := range hidden {
			sum += nn.WeightsHO[i][j] * hidden[j]
		}
		output[i] = sigmoid(sum)
	}

	// Get the predicted tag index (assuming maxIndex function exists)
	predictedTagIndex := maxIndex(output)

	// Check if predicted tag is unknown
	if _, ok := posTagVocab["<UNK>"]; ok && predictedTagIndex == posTagVocab["<UNK>"] {
		// Handle unknown tag (e.g., assign a specific probability)
		// You might want to adjust this based on your specific needs
		output[predictedTagIndex] = 0.5 // Example: Assign 0.5 probability
	}

	return output
}

// Activation function (sigmoid)
func sigmoid(x float64) float64 {
	return 1 / (1 + math.Exp(-x))
}
func PrintTable(tokens, pos, ner, phrase, dependency []string, message string) {
	fmt.Println("Token Index | Token        | pos Tag       | ner Tag       | dependency Tag      | phrase Tag")
	fmt.Println("------------|--------------|---------------|---------------|-----------------|--------------")

	// Print table rows
	for i := range tokens {
		fmt.Printf("%-12d| %-13s| %-14s| %-14s| %-16s| %-10s\n", i,
			tokens[i],
			pos[i],
			ner[i],
			dependency[i],
			phrase[i])
		fmt.Println(strings.Repeat("-", 90))

	}
	fmt.Println(message)

}

type SimpleNN struct {
	InputSize     int
	HiddenSize    int
	OutputSize    int
	WeightsIH     [][]float64 // Input to hidden weights
	WeightsHO     [][]float64 // Hidden to output weights
	Outputs       []float64   // Add this line to declare the Outputs field
	HiddenOutputs []float64
}

func NewSimpleNN(inputSize, hiddenSize, outputSize int) *SimpleNN {
	nn := &SimpleNN{
		InputSize:  inputSize,
		HiddenSize: hiddenSize,
		OutputSize: outputSize,
	}

	// Initialize Outputs field as an empty slice with the correct size
	nn.Outputs = make([]float64, outputSize)
	nn.HiddenOutputs = make([]float64, hiddenSize)
	nn.WeightsIH = make([][]float64, hiddenSize)
	for i := range nn.WeightsIH {
		nn.WeightsIH[i] = make([]float64, inputSize)
		for j := range nn.WeightsIH[i] {
			nn.WeightsIH[i][j] = rand.Float64()*2 - 1 // Initialize with random weights
		}
	}
	nn.WeightsHO = make([][]float64, outputSize)
	for i := range nn.WeightsHO {
		nn.WeightsHO[i] = make([]float64, hiddenSize)
		for j := range nn.WeightsHO[i] {
			nn.WeightsHO[i][j] = rand.Float64()*2 - 1
		}
	}
	return nn
}
func saveModelToGOB(model *SimpleNN, filePath string) error {
	file, err := os.Create(filePath)
	if err != nil {
		return err
	}
	defer file.Close()

	encoder := gob.NewEncoder(file)
	err = encoder.Encode(model)
	if err != nil {
		return err
	}

	return nil
}
func loadModelFromGOB(filePath string) (*SimpleNN, error) {
	file, err := os.Open(filePath)
	if err != nil {
		return nil, err
	}
	defer file.Close()

	decoder := gob.NewDecoder(file)
	var model SimpleNN
	err = decoder.Decode(&model)
	if err != nil {
		return nil, err
	}

	return &model, nil
}
